sales {
  # Spark配置
  spark {
    master = "yarn"
    app.name = "Sales Data ETL"
    executor.memory = "2g"
    executor.cores = 2
    executor.instances = 2
    driver.memory = "2g"
    shuffle.partitions = 200
  }
  
  # MySQL配置
  mysql {
    url = "jdbc:mysql://localhost:3306/sales_data_warehouse"
    driver = "com.mysql.cj.jdbc.Driver"
    user = "root"
    password = "root"
    batch.size = 10000
  }
  
  # Hive配置
  hive {
    warehouse = "hdfs://localhost:9000/user/hive/warehouse"
    metastore.uris = "thrift://localhost:9083"
    
    # 数据库配置
    database {
      ods = "sales_ods"
      dwd = "sales_dwd"
      dws = "sales_dws"
      ads = "sales_ads"
    }
    
    # 分区配置
    partition {
      type = "day"
      format = "yyyy-MM-dd"
    }
  }
  
  # Doris配置
  doris {
    jdbc.url = "jdbc:mysql://localhost:9030/sales_analysis"
    user = "root"
    password = "password"
    database = "sales_analysis"
  }
  
  # 表配置
  tables {
    users {
      source = "users"
      target = "dw_dim_users"
      type = "dimension"  # dimension or fact
      columns = [
        {source: "id", target: "user_id", type: "bigint"},
        {source: "name", target: "user_name", type: "string"},
        {source: "email", target: "email", type: "string"},
        {source: "created_time", target: "created_time", type: "timestamp"},
        {source: "update_time", target: "update_time", type: "timestamp"}
      ]
    }
    
    products {
      source = "products"
      target = "dw_dim_products"
      type = "dimension"
      columns = [
        {source: "id", target: "product_id", type: "bigint"},
        {source: "name", target: "product_name", type: "string"},
        {source: "category", target: "category", type: "string"},
        {source: "price", target: "price", type: "decimal(10,2)"},
        {source: "created_time", target: "created_time", type: "timestamp"},
        {source: "update_time", target: "update_time", type: "timestamp"}
      ]
    }
    
    orders {
      source = "orders"
      target = "dw_fact_orders"
      type = "fact"
      columns = [
        {source: "id", target: "order_id", type: "bigint"},
        {source: "user_id", target: "user_id", type: "bigint"},
        {source: "total_amount", target: "total_amount", type: "decimal(10,2)"},
        {source: "status", target: "status", type: "string"},
        {source: "created_time", target: "created_time", type: "timestamp"},
        {source: "update_time", target: "update_time", type: "timestamp"}
      ]
    }
    
    order_items {
      source = "order_items"
      target = "dw_fact_order_items"
      type = "fact"
      columns = [
        {source: "id", target: "item_id", type: "bigint"},
        {source: "order_id", target: "order_id", type: "bigint"},
        {source: "product_id", target: "product_id", type: "bigint"},
        {source: "quantity", target: "quantity", type: "int"},
        {source: "price", target: "price", type: "decimal(10,2)"},
        {source: "created_time", target: "created_time", type: "timestamp"}
      ]
    }
    
    inventory_updates {
      source = "inventory_updates"
      target = "dw_fact_inventory_updates"
      type = "fact"
      columns = [
        {source: "id", target: "update_id", type: "bigint"},
        {source: "product_id", target: "product_id", type: "bigint"},
        {source: "quantity_change", target: "quantity_change", type: "int"},
        {source: "update_type", target: "update_type", type: "string"},
        {source: "created_time", target: "created_time", type: "timestamp"}
      ]
    }
  }
  
  # ETL日期配置
  etl {
    date.format = "yyyy-MM-dd"
    partition.format = "yyyyMMdd"
    
    # 数据质量检查阈值
    quality {
      null.threshold = 0.1
      duplicate.threshold = 0.01
      dimension.integrity.threshold = 0.05
    }
  }
  
  # 数据生命周期配置
  lifecycle {
    ods.retention.days = 90        # ODS层数据保留90天
    dwd.retention.days = 180       # DWD层数据保留180天
    dws.day.retention.days = 365   # DWS层按天汇总数据保留365天
    dws.month.retention.years = 10 # DWS层按月汇总数据保留10年
  }
  
  # 数据处理窗口配置
  processing {
    default.window = "day"          # 默认按天处理
    max.retry.count = 3             # 最大重试次数
    retry.interval.seconds = 300    # 重试间隔(秒)
  }
  
  # 数据一致性检查阈值
  consistency {
    # 允许的DWD相对于ODS数据量的最小比例
    ods_to_dwd.threshold = 0.95
    # 允许的汇总指标误差率
    summary.error.threshold = 0.0001
  }

  # 任务配置
  job {
    # 增量同步配置
    incremental {
      enabled = true
      tracking_column = "update_time"  # 或者 created_time
    }
    
    # 并行度配置
    parallelism {
      partition_num = 10  # Spark分区数
      executor_memory = "2g"
      executor_cores = 2
    }

    # 错误处理
    error_handling {
      max_retries = 3
      retry_interval = 60  # seconds
      ignore_errors = false
    }
  }

  # 指标计算配置
  metrics {
    # 库存周转率计算
    inventory.turnover {
      window.days = 30
    }
    
    # 安全库存计算
    safety.stock {
      window.days = 90
      service.level = 0.95  # 95%服务水平
      lead.time.days = 7    # 7天提前期
    }
  }

  # 日志配置
  logging {
    level = "INFO"
    path = "logs"
    file.pattern = "sales-etl-%d{yyyy-MM-dd}.log"
    max.size = "100MB"
    max.files = 10
  }
} 